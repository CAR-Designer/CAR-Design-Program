How to get PDB ID's so you can put them in CSV list for batch download

Use this API
https://www.rcsb.org/docs/programmatic-access/web-apis-overview#search-api
______________________________________________________________________________

pip3 install requests

cd into a folder you would store general documents
nano
COPY PASTE THE FOLLOWING AND ADD YOUR OWN STUFF TO IT
ctrl+o + ENTER
pdb_request1.py
ENTER
ctrl+X

_____________________________________________________________________________________
GUIDE
https://search.rcsb.org/index.html#search-api

HOW TO GENERATE SEARCH REQUEST
https://youtu.be/0fmb_LMA_i8?si=9rz4Fx3CdaM-v1pR
_____________________________________________________________________________________
##THIS IS THE SEARCH QUERY INTO PYTHON TO GET PDB FILES FOR TCR'S
##SAVE IT AS A .py FILE

import requests
import json
import urllib.parse
import pandas as pd  # Import pandas

search_request = {  # Example search request (replace with your actua>
    "query": {
        "type": "group",
        "nodes": [
            {
                "type": "terminal",
                "service": "full_text",
                "parameters": {
                    "value": "t cell receptor"
                }
            },
            {
                "type": "terminal",
                "service": "text",
                "parameters": {
                    "attribute": "rcsb_entity_source_organism.ncbi_sc>
                    "value": "Homo sapiens",
                    "operator": "exact_match"
                }
            }
        ],
        "logical_operator": "and"
    },
    "request_options": {
        "return_all_hits": True,
        "results_content_type": ["experimental"],
        "sort": [
            {
               "sort_by": "score",
                "direction": "desc"
            }
        ],
        "scoring_strategy": "combined"
    },
    "return_type": "entry"
}

# URL encode the JSON request
url_encoded_request = urllib.parse.quote(json.dumps(search_request))
url = f"https://search.rcsb.org/rcsbsearch/v2/query?json={url_encoded>

response = requests.get(url)  # Alternatively, use requests.post(url,>

if response.status_code == 200:
    results = response.json()
    
    # Depending on the structure of results, you might need to adjust>
    # For example, if results is a list of entries:
    df = pd.DataFrame(results)  # Create a DataFrame from results
    
    # Save results to a CSV file
    df.to_csv('merged_data.csv', index=False)
    
    # Optionally, print the DataFrame to verify
    print(df)
else:
    print(f"Error: {response.status_code}")
    print(response.text)

__________________________________________________________________
#AFTER YOU GET MERGED_DATA.CSV FILE BACK WITH THE LIST OF PDB ID'S
#WE PARSE THE LIST AND FILTER EVERYTHING TO GET ONLY PDB ID'S

import csv

def csv_to_txt(csv_file, txt_file):
    with open(csv_file, 'r') as csvfile, open(txt_file, 'w') as txtfi>
        reader = csv.reader(csvfile)
        all_rows = []  # Accumulate the rows
        for row in reader:
            all_rows.extend(row)  # Add the elements of current row t>

        txtfile.write(','.join(all_rows)) # Write a single comma sepa>

csv_to_txt('merged_data.csv', 'output_txt_file.txt')

________________________________________________________________
#GET THE OUTPUT TEXT FILE 'out_txt_file.txt' AND USE THE API TO GET THE 70K PDB FILES BACK INTO A FOLDER

chmod +x ./batch_download.sh

./batch_download.sh -f ~/research/output_txt_file.txt -p





#After saving this is a .py file
#type into the terminal

python3 <name_of_python_file.py>
