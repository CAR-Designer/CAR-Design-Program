How to get PDB ID's so you can put them in CSV list for batch download

Original Guide-
https://www.rcsb.org/docs/programmatic-access/web-apis-overview#search-api
________________________________TRY THIS

GUIDE
https://search.rcsb.org/index.html#search-api

HOW TO GENERATE SEARCH REQUEST
https://youtu.be/0fmb_LMA_i8?si=9rz4Fx3CdaM-v1pR

Download Pandas first
_____________________________________________________________________________________
##THIS IS THE SEARCH QUERY INTO PYTHON TO GET PDB FILES FOR TCR'S
##SAVE IT AS A .py FILE

import requests
import json
import urllib.parse
import pandas as pd  # Import pandas

search_request = {  # Example search request (replace with your actual data)
    "query": {
        "type": "group",
        "nodes": [
            {
                "type": "terminal",
                "service": "full_text",
                "parameters": {
                    "value": "t cell receptor"
                }
            },
            {
                "type": "terminal",
                "service": "text",
                "parameters": {
                    "attribute": "rcsb_entity_source_organism.ncbi_scientific_name",
                    "value": "Homo sapiens",
                    "operator": "exact_match"
                }
            }
        ],
        "logical_operator": "and"
    },
    "request_options": {
        "return_all_hits": True,
        "results_content_type": ["experimental"],
        "sort": [
            {
                "sort_by": "score",
                "direction": "desc"
            }
        ],
        "scoring_strategy": "combined"
    },
    "return_type": "entry"
}

# URL encode the JSON request
url_encoded_request = urllib.parse.quote(json.dumps(search_request))
url = f"https://search.rcsb.org/rcsbsearch/v2/query?json={url_encoded_request}"

response = requests.get(url)  # Alternatively, use requests.post(url, ...)

if response.status_code == 200:
    results = response.json()
    
    # Depending on the structure of results, you might need to adjust this part
    # For example, if results is a list of entries:
    df = pd.DataFrame(results)  # Create a DataFrame from results
    
    # Save results to a CSV file
    df.to_csv('merged_data.csv', index=False)
    
    # Optionally, print the DataFrame to verify
    print(df)
else:
    print(f"Error: {response.status_code}")
    print(response.text)



import re

# Read the file (replace 'your_file.txt' with your actual file name)
with open('merged_data.csv', 'r') as file:
    lines = file.readlines()

# Regular expression to match the identifier value (e.g., '6JXR', '7FJD', etc.)
pattern = r"'identifier':\s?'([A-Za-z0-9]+)'"

# List to store extracted identifiers
identifiers = []

# Loop through each line in the file
for line in lines:
    # Search for the identifier pattern
    match = re.search(pattern, line)
    if match:
        # Append the identifier to the list
        identifiers.append(match.group(1))

# Create a comma-separated string of identifiers
identifier_string = ",".join(identifiers)

# Output the result
print("Extracted Identifiers:", identifier_string)

# Save the result to a .txt file
with open('idtxtoutputs.txt', 'w') as output_file:
    output_file.write(identifier_string)

______________________________________________________________________________
#GET THE OUTPUT TEXT FILE 'idtxtoutputs.txt' AND USE THE API TO GET THE 70K PDB FILES BACK INTO A FOLDER


#GET BATCH DOWNLOAD SCRIPT
wget https://www.rcsb.org/scripts/batch_download.sh

#GIVE IT PRIVILEDGES
chmod +x ./batch_download.sh

#RUN THE DOWNLOAD IN THE BACKGROUND OF EC2 SO YOU CAN CLOSE WINDOWS AND WALK AWAY AND COME BACK LATER SSH IN AND CHECK PROGRESS
#DONT STOP THE INSTANCE BUT YOU CAN CLOSE THE WINDOWS AFTER RUNNING THIS AND IT WILL DOWNLOAD TO IDTXTOUTPUTS.TXT AND BATCH_DOWNLOAD.OUT FOLDERS

nohup ./batch_download.sh -f ~/carfolder/idtxtoutputs.txt -p > batch_download.out 2>&1 &


#YOU CAN SSH IN
#TO CHECK THE PROGRESS LIVE
tail -f batch_download.out

#COUNT HOW MANY ARE OUTPUTTED CD INTO FOLDER AND RUN
ls -1 | wc -l

